{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does a bigram model work?\n",
    "\n",
    "The bigram model assumes that the probability of a word depends only on the word immediately preceding it. This is an application of the Markov assumption, which simplifies complex sequential data by reducing the dependency horizon to a fixed size (one in the case of a bigram model).\n",
    "\n",
    "Mathematically, for a sequence of words w1, w2,…, wn​, the probability of the sequence is approximated as:\n",
    "$$\n",
    "P(w_1,w_2,\\ldots,w_n) \\approx P(w_1) \\cdot P(w_2 \\mid w_1) \\cdot P(w_3 \\mid w_2) \\cdots P(w_n \\mid w_{n-1})\n",
    "$$\n",
    "\n",
    "Here, P(wi​ | wi−1​) is the probability of word wi​ given the preceding word wi−1​.\n",
    "\n",
    "Example\n",
    "\n",
    "Suppose we have a simple sentence:\n",
    "“I love programming languages.\"\n",
    "\n",
    "The bigrams extracted from this sentence are:\n",
    "\n",
    "    (\"I\", \"love\")\n",
    "\n",
    "    (\"love\", \"programming\")\n",
    "\n",
    "    (\"programming\", \"languages\")\n",
    "\n",
    "Using a bigram model, the probability of the sentence is computed as:\n",
    "\n",
    "$$\n",
    "P(\"I \\ love\\ programming\\ languages \\\") = P(\"I\") \\cdot P(\\text{love} \\mid \"I\") \\cdot P(\\text{programming} \\mid \\text{love}) \\cdot P(\\text{languages} \\mid \\text{programming})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = \"\"\"\n",
    "Welcome. This is a simple test text for model training.\n",
    "Hello my dear friend. This text is also a test and hope you like my example.\n",
    "We will test a simple bigram language model on this text.\n",
    "We will test and see how simple bigram langauge model behave.\n",
    "I am doing my best to learn through practice.\n",
    "Thank you for taking the time to read this.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    text= text.lower()\n",
    "    # Tokenize the text into words and punctuation\n",
    "    tokens = re.findall(r'\\b\\w+\\b|[^\\w\\s]', text, re.UNICODE)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:  ['welcome', '.', 'this', 'is', 'a', 'simple', 'test', 'text', 'for', 'model', 'training', '.', 'hello', 'my', 'dear', 'friend', '.', 'this', 'text', 'is', 'also', 'a', 'test', 'and', 'hope', 'you', 'like', 'my', 'example', '.', 'we', 'will', 'test', 'a', 'simple', 'bigram', 'language', 'model', 'on', 'this', 'text', '.', 'we', 'will', 'test', 'and', 'see', 'how', 'simple', 'bigram', 'langauge', 'model', 'behave', '.', 'i', 'am', 'doing', 'my', 'best', 'to', 'learn', 'through', 'practice', '.', 'thank', 'you', 'for', 'taking', 'the', 'time', 'to', 'read', 'this', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens=tokenize(text_data)\n",
    "print(\"Tokens: \", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens:  74\n"
     ]
    }
   ],
   "source": [
    "length = len(tokens)\n",
    "print(\"Number of tokens: \", length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram: ('welcome', '.'), Count: 1\n",
      "Bigram: ('.', 'this'), Count: 2\n",
      "Bigram: ('this', 'is'), Count: 1\n",
      "Bigram: ('is', 'a'), Count: 1\n",
      "Bigram: ('a', 'simple'), Count: 2\n",
      "Bigram: ('simple', 'test'), Count: 1\n",
      "Bigram: ('test', 'text'), Count: 1\n",
      "Bigram: ('text', 'for'), Count: 1\n",
      "Bigram: ('for', 'model'), Count: 1\n",
      "Bigram: ('model', 'training'), Count: 1\n",
      "Unigram: welcome, Count: 1\n",
      "Unigram: ., Count: 8\n",
      "Unigram: this, Count: 4\n",
      "Unigram: is, Count: 2\n",
      "Unigram: a, Count: 3\n",
      "Unigram: simple, Count: 3\n",
      "Unigram: test, Count: 4\n",
      "Unigram: text, Count: 3\n",
      "Unigram: for, Count: 2\n",
      "Unigram: model, Count: 3\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "bigram_counts = defaultdict(int)\n",
    "unigram_counts = defaultdict(int)\n",
    "# Count unigrams and bigrams\n",
    "for i in range(length-1):\n",
    "    w1=tokens[i]\n",
    "    w2=tokens[i+1]\n",
    "    bigram_counts[(w1,w2)] += 1\n",
    "    unigram_counts[w1] += 1\n",
    "unigram_counts[tokens[-1]] += 1\n",
    "for bigram,count in list(bigram_counts.items())[:10]:\n",
    "    print(f\"Bigram: {bigram}, Count: {count}\")\n",
    "for unigram,count in list(unigram_counts.items())[:10]:\n",
    "    print(f\"Unigram: {unigram}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Bigram probability calculation :\n",
    "\n",
    "bigram_probabilities will help to generate text, picking the next word based on the probability distribution given the current word. here we have simplified example of probability calculation.\n",
    "### Concrete Calculations\n",
    "\n",
    "$$\n",
    "P(\\text{\"my\"} \\mid \\text{\"hello\"}) \n",
    "= \\frac{\\text{count}(\\text{\"hello\"}, \\text{\"my\"})}{\\text{count}(\\text{\"hello\"})}\n",
    "= \\frac{2}{2}\n",
    "= 1.0\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{\"friend\"} \\mid \\text{\"my\"})\n",
    "= \\frac{\\text{count}(\\text{\"my\"}, \\text{\"friend\"})}{\\text{count}(\\text{\"my\"})}\n",
    "= \\frac{2}{3}\n",
    "\\approx 0.6667\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{\"cat\"} \\mid \\text{\"my\"})\n",
    "= \\frac{\\text{count}(\\text{\"my\"}, \\text{\"cat\"})}{\\text{count}(\\text{\"my\"})}\n",
    "= \\frac{1}{3}\n",
    "\\approx 0.3333\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram: ('welcome', '.'), Probability: 1.0000\n",
      "Bigram: ('.', 'this'), Probability: 0.2500\n",
      "Bigram: ('this', 'is'), Probability: 0.2500\n",
      "Bigram: ('is', 'a'), Probability: 0.5000\n",
      "Bigram: ('a', 'simple'), Probability: 0.6667\n",
      "Bigram: ('simple', 'test'), Probability: 0.3333\n",
      "Bigram: ('test', 'text'), Probability: 0.2500\n",
      "Bigram: ('text', 'for'), Probability: 0.3333\n",
      "Bigram: ('for', 'model'), Probability: 0.5000\n",
      "Bigram: ('model', 'training'), Probability: 0.3333\n"
     ]
    }
   ],
   "source": [
    "bigram_probabilities = defaultdict(float)\n",
    "for (w1,w2), count in bigram_counts.items():\n",
    "    bigram_probabilities[(w1,w2)] = count / unigram_counts[w1]\n",
    "for bigram, prob in list(bigram_probabilities.items())[:10]:\n",
    "    print(f\"Bigram: {bigram}, Probability: {prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start word:  for\n",
      "bigram_probabilities: defaultdict(<class 'float'>, {('welcome', '.'): 1.0, ('.', 'this'): 0.25, ('this', 'is'): 0.25, ('is', 'a'): 0.5, ('a', 'simple'): 0.6666666666666666, ('simple', 'test'): 0.3333333333333333, ('test', 'text'): 0.25, ('text', 'for'): 0.3333333333333333, ('for', 'model'): 0.5, ('model', 'training'): 0.3333333333333333, ('training', '.'): 1.0, ('.', 'hello'): 0.125, ('hello', 'my'): 1.0, ('my', 'dear'): 0.3333333333333333, ('dear', 'friend'): 1.0, ('friend', '.'): 1.0, ('this', 'text'): 0.5, ('text', 'is'): 0.3333333333333333, ('is', 'also'): 0.5, ('also', 'a'): 1.0, ('a', 'test'): 0.3333333333333333, ('test', 'and'): 0.5, ('and', 'hope'): 0.5, ('hope', 'you'): 1.0, ('you', 'like'): 0.5, ('like', 'my'): 1.0, ('my', 'example'): 0.3333333333333333, ('example', '.'): 1.0, ('.', 'we'): 0.25, ('we', 'will'): 1.0, ('will', 'test'): 1.0, ('test', 'a'): 0.25, ('simple', 'bigram'): 0.6666666666666666, ('bigram', 'language'): 0.5, ('language', 'model'): 1.0, ('model', 'on'): 0.3333333333333333, ('on', 'this'): 1.0, ('text', '.'): 0.3333333333333333, ('and', 'see'): 0.5, ('see', 'how'): 1.0, ('how', 'simple'): 1.0, ('bigram', 'langauge'): 0.5, ('langauge', 'model'): 1.0, ('model', 'behave'): 0.3333333333333333, ('behave', '.'): 1.0, ('.', 'i'): 0.125, ('i', 'am'): 1.0, ('am', 'doing'): 1.0, ('doing', 'my'): 1.0, ('my', 'best'): 0.3333333333333333, ('best', 'to'): 1.0, ('to', 'learn'): 0.5, ('learn', 'through'): 1.0, ('through', 'practice'): 1.0, ('practice', '.'): 1.0, ('.', 'thank'): 0.125, ('thank', 'you'): 1.0, ('you', 'for'): 0.5, ('for', 'taking'): 0.5, ('taking', 'the'): 1.0, ('the', 'time'): 1.0, ('time', 'to'): 1.0, ('to', 'read'): 0.5, ('read', 'this'): 1.0, ('this', '.'): 0.25})\n",
      "Generated sentence:  for model behave . this text . we will test\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_sentence(bigram_probabilities, start_word, max_length=8):\n",
    "    sentence = [start_word]\n",
    "    for _ in range(max_length - 1):\n",
    "        next_word_candidates = [(w2, prob) for (w1, w2), prob in bigram_probabilities.items() if w1 == sentence[-1]]\n",
    "        if not next_word_candidates:\n",
    "            break\n",
    "        next_words, probs = zip(*next_word_candidates)\n",
    "        next_word = random.choices(next_words, weights=probs)[0] # Choose the next word based on the probabilities\n",
    "        sentence.append(next_word)\n",
    "    return ' '.join(sentence)\n",
    "start_word = random.choice(tokens) # Choose a random start word from the tokens but you can also change it to any word in the text\n",
    "#choose random start word from the tokens, if its punctuation, choose the next word\n",
    "#if tokens.index(start_word) == len(tokens)-1 or not tokens[tokens.index(start_word)+1].isalpha():\n",
    "#    start_word = random.choice(tokens[:-1])\n",
    "\n",
    "print(\"Start word: \", start_word)\n",
    "generated_sentence = generate_sentence(bigram_probabilities, start_word, max_length=10)\n",
    "print(\"bigram_probabilities:\", bigram_probabilities)\n",
    "print(\"Generated sentence: \", generated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
